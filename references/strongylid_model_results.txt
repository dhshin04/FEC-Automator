200 images used for training, validation, and test sets (150 train, 25 val, 25 test)
    - 60 training set images were generated using data augmentation techniques, implemented
      using Roboflow (specifics under .../strongylid_dataset/README.roboflow.txt)

Data Augmentation used during training: random rotate by 90, random horizontal flip

Overview: The model fine-tuned directly on torchvision's pretrained Faster R-CNN
    outperformed the model tuned on the custom general egg detection model. However, there is
    reasonable suspicion that the former model may be more prone to overfitting, suggested by
    its unreasonably high accuracy despite a small training set. By contrast, the latter 
    model displayed slower convergence, likely due to conflicts with features of general 
    fecal eggs learned during fine-tuning, potentially preventing overfitting. It is 
    important to note that this is just a hypothesis, and further research is required 
    using a larger dataset.


Model Fine-Tuned on Custom General Egg Detection Model:
    Validation Performance
    Precision: 0.7419, Recall: 0.9744, Elapsed Time: 4.23s
    Fecal Egg Count Accuracy: 73.50%

    mAP Results:
    mAP@0.5: 0.9098
    mAP@0.5-0.95: 0.6422

    Test Performance
    Precision: 0.9247, Recall: 0.9786, Elapsed Time: 3.38s
    Fecal Egg Count Accuracy: 98.25%

    mAP Results:
    mAP@0.5: 0.9302
    mAP@0.5-0.95: 0.6172

    Average FEC Accuracy on Untrained Data (Validation + Test): 85.877%


Model Fine-Tuned on torchvision's pretrained Faster R-CNN:
    Validation Performance
    Precision: 0.9240, Recall: 0.9711, Elapsed Time: 4.19s
    Fecal Egg Count Accuracy: 93.13%

    mAP Results:
    mAP@0.5: 0.9184
    mAP@0.5-0.95: 0.6379

    Test Performance
    Precision: 0.8632, Recall: 0.9557, Elapsed Time: 3.36s
    Fecal Egg Count Accuracy: 95.12%

    mAP Results:
    mAP@0.5: 0.9415
    mAP@0.5-0.95: 0.6055

    Average FEC Accuracy on Untrained Data (Validation + Test): 94.121%


Hyperparameters for Training
    train_batch = 2                 
    cv_batch = 8                    
    accumulation_size = 1           
    learning_rate = 1e-5            
    epochs = 30                    
    warmup_step = 10                
    weight_decay = 0.0005           
    T_max = epochs                  
    eta_min = learning_rate / 30  

    AdamW
    SequentialLR with CosineAnnealingLR + LambdaLR
    GradScaler()  


Hyperparameters for Inference
    iou_threshold = 0.5
    confidence_threshold = 0.3      # Best for both strongylid models with and without general model fine-tuning 
    nms_threshold = 0.3